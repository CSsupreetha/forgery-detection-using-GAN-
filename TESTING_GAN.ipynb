{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VfR1Ta6bXkvlrC9aYMYKokSJ5YYtAdAI",
      "authorship_tag": "ABX9TyPYRJtX/m3kuRTYIbH2tsMX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIBngygypKIz",
        "outputId": "b0af534a-1a8b-4f64-a251-2730ea1dd39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh1MMUr31jNJ",
        "outputId": "b9073e6a-72be-47fa-b7e9-da8cc4733143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "00\n",
            "\n",
            "0\n",
            "0\n",
            "10\n",
            "\n",
            "11\n",
            "\n",
            "01\n",
            "\n",
            "01\n",
            "\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "00\n",
            "\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "01\n",
            "\n",
            "0\n",
            "0\n",
            "11\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "11\n",
            "\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "01\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "00\n",
            "\n",
            "0\n",
            "1\n",
            "01\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "Total Count 0: 50, Total Count 1: 50\n",
            "Th: 0.503\n"
          ]
        }
      ],
      "source": [
        "#DC GAN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define hyperparameters (same as used during training)\n",
        "image_size = 64\n",
        "image_channels = 3\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for preprocessing the input image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define the Discriminator class\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_channels, hidden_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: image_channels x 64 x 64\n",
        "            nn.Conv2d(image_channels, hidden_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Input: hidden_dim x 32 x 32\n",
        "            nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Input: hidden_dim * 2 x 16 x 16\n",
        "            nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Input: hidden_dim * 4 x 8 x 8\n",
        "            nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_dim * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Input: hidden_dim * 8 x 4 x 4\n",
        "            nn.Conv2d(hidden_dim * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid()  # Output: 1 x 1 x 1\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "      return self.main(input)\n",
        "\n",
        "# Load the pre-trained discriminator model on the CPU\n",
        "discriminator = Discriminator(image_channels).to(device)\n",
        "discriminator.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/GAN_gate/GATE/dc_models/netD_epoch_430.pth\", map_location=device))\n",
        "discriminator.eval()  # Set the discriminator to evaluation mode\n",
        "\n",
        "# Define the classify_image function\n",
        "def classify_image(image_path, threshold=0.8):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Use the discriminator to classify the image\n",
        "    with torch.no_grad():\n",
        "        output = discriminator(image)\n",
        "\n",
        "    # Apply a sigmoid activation function to get the probabilities\n",
        "    probabilities = torch.sigmoid(output)\n",
        "\n",
        "    # Compute the mean probability over the batch\n",
        "    mean_probability = torch.mean(probabilities).item()\n",
        "\n",
        "    # Check if the image is fake or genuine based on the mean probability and threshold\n",
        "    if mean_probability > threshold:\n",
        "        return \"1\"\n",
        "    else:\n",
        "        return \"0\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Directory containing the image files\n",
        "image_dir = \"/content/drive/MyDrive/Colab Notebooks/GAN_gate/GATE/Fake_images/Retouching_images\"  # Replace with the path to your image directory\n",
        "\n",
        "# Threshold for classification\n",
        "threshold = 0.503\n",
        "\n",
        "# List all image files in the directory\n",
        "image_files = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
        "\n",
        "# Initialize counters for '0's and '1's\n",
        "total_count_0 = 0\n",
        "total_count_1 = 0\n",
        "\n",
        "# Define a function to classify an image and return counts\n",
        "def classify_and_count(image_path):\n",
        "    result = classify_image(image_path, threshold=threshold)\n",
        "\n",
        "    # Count the number of '0's and '1's in the classification result\n",
        "    count_0 = result.count('0')\n",
        "    count_1 = result.count('1')\n",
        "\n",
        "    # Print the individual counts for each image\n",
        "    #print(f\"The image at {image_path} is classified as: {result}\")\n",
        "    print(result)\n",
        "    #print(f\"Count 0: {count_0}, Count 1: {count_1}\")\n",
        "\n",
        "    return count_0, count_1\n",
        "\n",
        "# Import the multiprocessing module with Pool\n",
        "import multiprocessing\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a multiprocessing pool with the number of desired processes\n",
        "    num_processes = multiprocessing.cpu_count()  # You can adjust this as needed\n",
        "    pool = multiprocessing.Pool(processes=num_processes)\n",
        "\n",
        "    # Use the pool to classify images in parallel and collect the results\n",
        "    results = pool.map(classify_and_count, image_files)\n",
        "\n",
        "    # Close the pool to release resources\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    # Sum up the counts from all images\n",
        "    total_count_0 = sum(count_0 for count_0, _ in results)\n",
        "    total_count_1 = sum(count_1 for _, count_1 in results)\n",
        "\n",
        "    # Display the total counts\n",
        "    print(f\"Total Count 0: {total_count_0}, Total Count 1: {total_count_1}\")\n",
        "    print(\"Th:\",threshold)\n",
        "\n",
        "# Test an input image for different thresholds\n",
        "#image_path_to_test = \"/content/drive/MyDrive/GATE/Generated_dc/epoch_348_image_001.png\"  # Replace with the path of the image you want to test\n",
        "#thresholds = [0.5 + i * 0.01 for i in range(0, 20)]\n",
        "\n",
        "#for threshold in thresholds:\n",
        "    #result = classify_image(image_path_to_test, threshold=threshold)\n",
        "    #print(f\"The image is classified as: {result} (Threshold: {threshold})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc7IBamDWn5C",
        "outputId": "bf6dff9c-dda7-4b7a-adc1-03d096e5f037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01\n",
            "\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "10\n",
            "\n",
            "10\n",
            "\n",
            "10\n",
            "\n",
            "00\n",
            "\n",
            "00\n",
            "\n",
            "0\n",
            "1\n",
            "00\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "10\n",
            "\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "10\n",
            "\n",
            "1\n",
            "0\n",
            "11\n",
            "\n",
            "11\n",
            "\n",
            "0\n",
            "1\n",
            "10\n",
            "\n",
            "11\n",
            "\n",
            "01\n",
            "\n",
            "01\n",
            "\n",
            "01\n",
            "\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "11\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "10\n",
            "\n",
            "00\n",
            "\n",
            "0\n",
            "1\n",
            "11\n",
            "\n",
            "01\n",
            "\n",
            "11\n",
            "\n",
            "10\n",
            "\n",
            "01\n",
            "\n",
            "1\n",
            "1\n",
            "01\n",
            "\n",
            "10\n",
            "\n",
            "00\n",
            "\n",
            "10\n",
            "\n",
            "1\n",
            "1\n",
            "1\n",
            "Total Count 0: 41, Total Count 1: 59\n",
            "thr: 0.3\n"
          ]
        }
      ],
      "source": [
        "#W gan\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define hyperparameters (same as used during training)\n",
        "image_size = 64\n",
        "image_channels = 3\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for preprocessing the input image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define the critic (discriminator) network\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.disc = nn.Sequential(\n",
        "            #size = 3*64*64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), # Size : 32*32\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(features_d, features_d*2, kernel_size = 4, stride = 2, padding = 1), # size = 16*16\n",
        "            nn.BatchNorm2d(features_d*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(features_d*2, features_d*4, kernel_size = 4, stride = 2, padding = 1), # size = 8*8\n",
        "            nn.BatchNorm2d(features_d*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(features_d*4, features_d*8, kernel_size = 4, stride = 2, padding = 1), # size = 4*4\n",
        "            nn.BatchNorm2d(features_d*8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0) #1*1\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "# Load the pre-trained discriminator model on the CPU\n",
        "# You need to specify the number of features for the discriminator\n",
        "features_d = 64  # You can adjust this value as needed\n",
        "\n",
        "# Create an instance of the Discriminator class\n",
        "discriminator = Discriminator(image_channels, features_d).to(device)\n",
        "#discriminator = Discriminator(image_channels).to(device)\n",
        "discriminator.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/GAN_gate/GATE/wg_models/discriminator_epoch750.pth\", map_location=device))\n",
        "discriminator.eval()  # Set the discriminator to evaluation mode\n",
        "\n",
        "# Define the classify_image function\n",
        "def classify_image(image_path, threshold=0.8):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Use the discriminator to classify the image\n",
        "    with torch.no_grad():\n",
        "        output = discriminator(image)\n",
        "\n",
        "    # Apply a sigmoid activation function to get the probabilities\n",
        "    probabilities = torch.sigmoid(output)\n",
        "\n",
        "    # Compute the mean probability over the batch\n",
        "    mean_probability = torch.mean(probabilities).item()\n",
        "\n",
        "    # Check if the image is fake or genuine based on the mean probability and threshold\n",
        "    if mean_probability > threshold:\n",
        "        return \"1\"\n",
        "    else:\n",
        "        return \"0\"\n",
        "import os\n",
        "\n",
        "# Directory containing the image files\n",
        "image_dir = \"/content/drive/MyDrive/Colab Notebooks/GAN_gate/GATE/Fake_images/Retouching_images\"  # Replace with the path to your image directory\n",
        "\n",
        "# Threshold for classification\n",
        "threshold = 0.3\n",
        "\n",
        "# List all image files in the directory\n",
        "image_files = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
        "\n",
        "# Initialize counters for '0's and '1's\n",
        "total_count_0 = 0\n",
        "total_count_1 = 0\n",
        "\n",
        "# Define a function to classify an image and return counts\n",
        "def classify_and_count(image_path):\n",
        "    result = classify_image(image_path, threshold=threshold)\n",
        "\n",
        "    # Count the number of '0's and '1's in the classification result\n",
        "    count_0 = result.count('0')\n",
        "    count_1 = result.count('1')\n",
        "\n",
        "    # Print the individual counts for each image\n",
        "    #print(f\"The image at {image_path} is classified as: {result}\")\n",
        "    #print(f\"Count 0: {count_0}, Count 1: {count_1}\")\n",
        "    print(result)\n",
        "    return count_0, count_1\n",
        "\n",
        "# Import the multiprocessing module with Pool\n",
        "import multiprocessing\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a multiprocessing pool with the number of desired processes\n",
        "    num_processes = multiprocessing.cpu_count()  # You can adjust this as needed\n",
        "    pool = multiprocessing.Pool(processes=num_processes)\n",
        "\n",
        "    # Use the pool to classify images in parallel and collect the results\n",
        "    results = pool.map(classify_and_count, image_files)\n",
        "\n",
        "    # Close the pool to release resources\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    # Sum up the counts from all images\n",
        "    total_count_0 = sum(count_0 for count_0, _ in results)\n",
        "    total_count_1 = sum(count_1 for _, count_1 in results)\n",
        "\n",
        "    # Display the total counts\n",
        "    print(f\"Total Count 0: {total_count_0}, Total Count 1: {total_count_1}\")\n",
        "    print(\"thr:\",threshold)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test an input image for different thresholds\n",
        "#image_path_to_test = \"/content/drive/MyDrive/TEST/Copy of O_0001.jpg\"  # Replace with the path of the image you want to test\n",
        "#thresholds = [0.5 + i * 0.01 for i in range(0, 20)]\n",
        "\n",
        "#for threshold in thresholds:\n",
        "    #result = classify_image(image_path_to_test, threshold=threshold)\n",
        "    #print(f\"The image is classified as: {result} (Threshold: {threshold})\")\n"
      ]
    }
  ]
}